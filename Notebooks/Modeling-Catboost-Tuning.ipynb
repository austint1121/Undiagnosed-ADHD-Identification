{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Catboost Tuning\n",
    "## Summary\n",
    "In this notebook I will primarily tune a Catboost model using [Optuna](https://optuna.readthedocs.io/en/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Data and Required Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from catboost import CatBoostClassifier, metrics, cv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, accuracy_score\n",
    "import optuna\n",
    "\n",
    "from functions import metrics as custom_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Training Data\n",
    "X_train = pd.read_csv('../Data/train/X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('../Data/train/y_train.csv', index_col=0)\n",
    "\n",
    "# Testing Data\n",
    "X_test = pd.read_csv('../Data/test/X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('../Data/test/y_test.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Initiate Over sampler\n",
    "ros = RandomOverSampler(random_state=15)\n",
    "\n",
    "# Applying ONLY to training set to prevent data leakage.\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizing with the Optuna\n",
    "For the hyperparameter tuning process, I'll be using the [Optuna](https://optuna.readthedocs.io/en/stable/index.html) library. I'll also be making use of Catboost's cross-validation in selecting the best model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Optuna requires us to define the \"objective function\". Which will be called upon each iteration during our \"trials\"\n",
    "def objective(trial):\n",
    "    # Dict of Parameters to check\n",
    "    param = {\n",
    "        # Metric used for model optimization\n",
    "        'loss_function':trial.suggest_categorical('loss_function', ['Logloss', 'CrossEntropy']),\n",
    "\n",
    "        # The maximum number of trees that can be built.\n",
    "        'iterations':trial.suggest_categorical('iterations', [100,200,300,500,1000]),\n",
    "\n",
    "        # learning rate for gradient descent calculations.\n",
    "        'learning_rate':trial.suggest_float(\"learning_rate\", 0.001, 0.3),\n",
    "\n",
    "        # Coefficient at the L2 regularization term of the cost function.\n",
    "        'l2_leaf_reg': trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 100),\n",
    "\n",
    "        # Affects the speed and regularization of tree\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "\n",
    "        # The amount of randomness to use for scoring splits.\n",
    "        'random_strength':trial.suggest_int(\"random_strength\", 1,10),\n",
    "\n",
    "        # The number of splits for numerical features.\n",
    "        'max_bin':trial.suggest_categorical('max_bin', [4,5,6,8,10,20,30]),\n",
    "\n",
    "        # Allowed depth of tree.\n",
    "        \"depth\": trial.suggest_int(\"max_depth\", 2,16),\n",
    "\n",
    "        # Defines how to perform greedy tree construction.\n",
    "        'grow_policy':trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
    "\n",
    "        # The minimum number of training samples in a leaf.\n",
    "        'min_data_in_leaf':trial.suggest_int(\"min_data_in_leaf\", 1,10),\n",
    "\n",
    "        # Only OHE encodes features if the number of unique values will be <= the parameter vale.\n",
    "        'one_hot_max_size':trial.suggest_categorical('one_hot_max_size', [5,10,12,100]),\n",
    "    }\n",
    "\n",
    "    # Certain parameters are \"subparameters\" and can only be set if their parent parameter has a certain value.\n",
    "\n",
    "    # Bootstrap types\n",
    "    if param['bootstrap_type'] == \"Bayesian\":\n",
    "\n",
    "        # Use Baysesian bootstrapping to assign random weights to objects.\n",
    "        param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "\n",
    "    elif param['bootstrap_type'] in ['Bernoulli', 'MVS']:\n",
    "        # Sample rate for bagging using Bernoulli/MVS type\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n",
    "\n",
    "    # Grow policy params\n",
    "    if param['grow_policy'] != 'SymmetricTree':\n",
    "\n",
    "        # The minimum number of training samples in a leaf.\n",
    "        param['min_data_in_leaf'] = trial.suggest_int('min_data_in_leaf', 1, 10)\n",
    "\n",
    "        if param['grow_policy'] == 'LossGuide':\n",
    "\n",
    "            # The maximum number of leafs in the tree.\n",
    "            param['max_leaves'] = trial.suggest_int('max_leaves', 16, 64)\n",
    "\n",
    "\n",
    "    # Creates the trial model with parameters specified above.\n",
    "    trial_model = CatBoostClassifier(**param)\n",
    "\n",
    "    # Fit the training model on training data\n",
    "    trial_model.fit(X_train_os,\n",
    "                    y_train_os,\n",
    "                    eval_set=[(X_test, test_labels)],\n",
    "                    verbose=0, # Stops Catboost from printing training results.\n",
    "                    early_stopping_rounds=10 # Specify rounds of no improvement needed before stopping\n",
    "                    )\n",
    "\n",
    "    # Create predictions for test set\n",
    "    preds = trial_model.predict(X_test)\n",
    "\n",
    "    # Calculate recall score\n",
    "    accuracy = accuracy_score(test_labels, preds)\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2021-11-30 16:04:45,226]\u001B[0m A new study created in memory with name: no-name-432bb4b3-d4ea-4671-9b98-1a1f078f6ee4\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:05:48,980]\u001B[0m Trial 0 finished with value: 0.9035812672176309 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 300, 'learning_rate': 0.06362316550101686, 'l2_leaf_reg': 0.00017425311439678997, 'bootstrap_type': 'Bernoulli', 'random_strength': 5, 'max_bin': 5, 'max_depth': 12, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 6, 'one_hot_max_size': 10, 'subsample': 0.368893737002423}. Best is trial 0 with value: 0.9035812672176309.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:06:17,347]\u001B[0m Trial 1 finished with value: 0.9084648134234911 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 500, 'learning_rate': 0.14245109888226404, 'l2_leaf_reg': 2.5711131526874883e-05, 'bootstrap_type': 'Bernoulli', 'random_strength': 6, 'max_bin': 4, 'max_depth': 6, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'subsample': 0.2131524546504172}. Best is trial 1 with value: 0.9084648134234911.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:06:28,485]\u001B[0m Trial 2 finished with value: 0.905709992486852 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 300, 'learning_rate': 0.24814326348440027, 'l2_leaf_reg': 0.00016125553809523475, 'bootstrap_type': 'Bernoulli', 'random_strength': 9, 'max_bin': 10, 'max_depth': 7, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 4, 'one_hot_max_size': 5, 'subsample': 0.663844290314844}. Best is trial 1 with value: 0.9084648134234911.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:06:50,145]\u001B[0m Trial 3 finished with value: 0.8946907087402955 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 1000, 'learning_rate': 0.17288603084775925, 'l2_leaf_reg': 1.4889909557099588e-07, 'bootstrap_type': 'MVS', 'random_strength': 5, 'max_bin': 6, 'max_depth': 12, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 3, 'one_hot_max_size': 5, 'subsample': 0.3210887347583721}. Best is trial 1 with value: 0.9084648134234911.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:07:00,707]\u001B[0m Trial 4 finished with value: 0.8770348109191084 and parameters: {'loss_function': 'Logloss', 'iterations': 300, 'learning_rate': 0.02465163818516301, 'l2_leaf_reg': 30.973864146149182, 'bootstrap_type': 'Bayesian', 'random_strength': 10, 'max_bin': 4, 'max_depth': 2, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 3, 'one_hot_max_size': 10, 'bagging_temperature': 2.654742100577704}. Best is trial 1 with value: 0.9084648134234911.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:08:12,106]\u001B[0m Trial 5 finished with value: 0.8739043325820185 and parameters: {'loss_function': 'Logloss', 'iterations': 300, 'learning_rate': 0.005348643153839381, 'l2_leaf_reg': 69.98542486850995, 'bootstrap_type': 'Bernoulli', 'random_strength': 2, 'max_bin': 5, 'max_depth': 12, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 4, 'one_hot_max_size': 10, 'subsample': 0.44838812410593176}. Best is trial 1 with value: 0.9084648134234911.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 16:10:24,494]\u001B[0m Trial 6 finished with value: 0.9077134986225895 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 1000, 'learning_rate': 0.27896640509282394, 'l2_leaf_reg': 0.018763742440532298, 'bootstrap_type': 'Bayesian', 'random_strength': 7, 'max_bin': 20, 'max_depth': 15, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 3, 'one_hot_max_size': 100, 'bagging_temperature': 2.1885395435542474}. Best is trial 1 with value: 0.9084648134234911.\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/y8/vq429chs6djb0hl0y0zp5fg00000gn/T/ipykernel_16441/1481051354.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Running 100 trials, setting a timeout value of 15 minutes to prevent my computer from exploding.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mstudy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobjective\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_trials\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m900\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Number of finished trials: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstudy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/optuna/study/study.py\u001B[0m in \u001B[0;36moptimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    398\u001B[0m             )\n\u001B[1;32m    399\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 400\u001B[0;31m         _optimize(\n\u001B[0m\u001B[1;32m    401\u001B[0m             \u001B[0mstudy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    402\u001B[0m             \u001B[0mfunc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     64\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mn_jobs\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m             _optimize_sequential(\n\u001B[0m\u001B[1;32m     67\u001B[0m                 \u001B[0mstudy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m                 \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 163\u001B[0;31m             \u001B[0mtrial\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_run_trial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstudy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    164\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m             \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    212\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 213\u001B[0;31m         \u001B[0mvalue_or_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    214\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mexceptions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrialPruned\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m         \u001B[0;31m# TODO(mamu): Handle multi-objective cases.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/y8/vq429chs6djb0hl0y0zp5fg00000gn/T/ipykernel_16441/3357089647.py\u001B[0m in \u001B[0;36mobjective\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[0;31m# Fit the training model on training data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m     trial_model.fit(X_train_os,\n\u001B[0m\u001B[1;32m     68\u001B[0m                     \u001B[0my_train_os\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m                     \u001B[0meval_set\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/catboost/core.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   4673\u001B[0m             \u001B[0mCatBoostClassifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_is_compatible_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'loss_function'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4674\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4675\u001B[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001B[0m\u001B[1;32m   4676\u001B[0m                   \u001B[0meval_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogging_level\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_description\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose_eval\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric_period\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4677\u001B[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/catboost/core.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   1995\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mlog_fixup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_cout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_cerr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1996\u001B[0m             \u001B[0mplot_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_get_train_dir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1997\u001B[0;31m             self._train(\n\u001B[0m\u001B[1;32m   1998\u001B[0m                 \u001B[0mtrain_pool\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1999\u001B[0m                 \u001B[0mtrain_params\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"eval_sets\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/catboost/core.py\u001B[0m in \u001B[0;36m_train\u001B[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001B[0m\n\u001B[1;32m   1426\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1427\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_clear_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1428\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_object\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_clear_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_object\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0minit_model\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1429\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_trained_model_attributes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1430\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m_catboost.pyx\u001B[0m in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_catboost.pyx\u001B[0m in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate a \"trial\" object and specify we want to MAXIMIZE the value being returned by the obj function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Running 100 trials, setting a timeout value of 15 minutes to prevent my computer from exploding.\n",
    "study.optimize(objective, n_trials=100, timeout=900)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "trial = study.best_trial\n",
    "\n",
    "# \"Prettify\" our trial results\n",
    "print(\"Best trial:\")\n",
    "\n",
    "# Print metric value achieved from best trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "# Print all parameters from best trial\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a model with the parameters from our best trial\n",
    "final_model = CatBoostClassifier(verbose=False, **trial.params)\n",
    "final_model.fit(X_train_os, y_train_os)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show custom metrics\n",
    "final_results = custom_metric(test_labels, final_model.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}