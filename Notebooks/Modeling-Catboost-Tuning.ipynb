{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Catboost Tuning\n",
    "## Summary\n",
    "In this notebook I will be tuning a Catboost model using [Optuna](https://optuna.readthedocs.io/en/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Data and Required Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from catboost import CatBoostClassifier, metrics, cv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, accuracy_score\n",
    "import optuna\n",
    "\n",
    "from functions import metrics as custom_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Training Data\n",
    "X_train = pd.read_csv('../Data/train/X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('../Data/train/y_train.csv', index_col=0)\n",
    "\n",
    "# Testing Data\n",
    "X_test = pd.read_csv('../Data/test/X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('../Data/test/y_test.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Initiate Over sampler\n",
    "ros = RandomOverSampler(random_state=15)\n",
    "\n",
    "# Applying ONLY to training set to prevent data leakage.\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizing with the Optuna\n",
    "For the hyperparameter tuning process, I'll be using the [Optuna](https://optuna.readthedocs.io/en/stable/index.html) library."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Optuna requires us to define the \"objective function\". Which will be called upon each iteration during our \"trials\"\n",
    "def objective(trial):\n",
    "    # Dict of Parameters to check\n",
    "    param = {\n",
    "        # Metric used for model optimization\n",
    "        'loss_function':trial.suggest_categorical('loss_function', ['Logloss', 'CrossEntropy']),\n",
    "\n",
    "        # The maximum number of trees that can be built.\n",
    "        'iterations':trial.suggest_categorical('iterations', [100,200,300,500,1000]),\n",
    "\n",
    "        # learning rate for gradient descent calculations.\n",
    "        'learning_rate':trial.suggest_float(\"learning_rate\", 0.001, 0.3),\n",
    "\n",
    "        # Coefficient at the L2 regularization term of the cost function.\n",
    "        'l2_leaf_reg': trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 100),\n",
    "\n",
    "        # Affects the speed and regularization of tree\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "\n",
    "        # The amount of randomness to use for scoring splits.\n",
    "        'random_strength':trial.suggest_int(\"random_strength\", 1,10),\n",
    "\n",
    "        # The number of splits for numerical features.\n",
    "        'max_bin':trial.suggest_categorical('max_bin', [4,5,6,8,10,20,30]),\n",
    "\n",
    "        # Allowed depth of tree.\n",
    "        \"depth\": trial.suggest_int(\"max_depth\", 2,16),\n",
    "\n",
    "        # Defines how to perform greedy tree construction.\n",
    "        'grow_policy':trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
    "\n",
    "        # The minimum number of training samples in a leaf.\n",
    "        'min_data_in_leaf':trial.suggest_int(\"min_data_in_leaf\", 1,10),\n",
    "\n",
    "        # Only OHE encodes features if the number of unique values will be <= the parameter vale.\n",
    "        'one_hot_max_size':trial.suggest_categorical('one_hot_max_size', [5,10,12,100]),\n",
    "    }\n",
    "\n",
    "    # Certain parameters are \"subparameters\" and can only be set if their parent parameter has a certain value.\n",
    "\n",
    "    # Bootstrap types\n",
    "    if param['bootstrap_type'] == \"Bayesian\":\n",
    "\n",
    "        # Use Baysesian bootstrapping to assign random weights to objects.\n",
    "        param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "\n",
    "    elif param['bootstrap_type'] in ['Bernoulli', 'MVS']:\n",
    "        # Sample rate for bagging using Bernoulli/MVS type\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n",
    "\n",
    "    # Grow policy params\n",
    "    if param['grow_policy'] != 'SymmetricTree':\n",
    "\n",
    "        # The minimum number of training samples in a leaf.\n",
    "        param['min_data_in_leaf'] = trial.suggest_int('min_data_in_leaf', 1, 10)\n",
    "\n",
    "        if param['grow_policy'] == 'LossGuide':\n",
    "\n",
    "            # The maximum number of leafs in the tree.\n",
    "            param['max_leaves'] = trial.suggest_int('max_leaves', 16, 64)\n",
    "\n",
    "\n",
    "    # Creates the trial model with parameters specified above.\n",
    "    trial_model = CatBoostClassifier(**param)\n",
    "\n",
    "    # Fit the training model on training data\n",
    "    trial_model.fit(X_train_os,\n",
    "                    y_train_os,\n",
    "                    eval_set=[(X_test, y_test)],\n",
    "                    verbose=0, # Stops Catboost from printing training results.\n",
    "                    early_stopping_rounds=10 # Specify rounds of no improvement needed before stopping\n",
    "                    )\n",
    "\n",
    "    # Create predictions for test set\n",
    "    preds = trial_model.predict(X_test)\n",
    "\n",
    "    # Calculate recall score\n",
    "    roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    return roc_auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2021-12-01 13:15:20,262]\u001B[0m A new study created in memory with name: no-name-89e1714a-9d70-4350-8f0a-4f6333145aec\u001B[0m\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.\u001B[32m[I 2021-12-01 13:16:11,912]\u001B[0m Trial 0 finished with value: 0.8137827975469601 and parameters: {'loss_function': 'Logloss', 'iterations': 1000, 'learning_rate': 0.27853655059569205, 'l2_leaf_reg': 0.06366426493797718, 'bootstrap_type': 'Bernoulli', 'random_strength': 4, 'max_bin': 4, 'max_depth': 9, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 3, 'one_hot_max_size': 12, 'subsample': 0.7397134390129195}. Best is trial 0 with value: 0.8137827975469601.\u001B[0m\n",
      "\u001B[32m[I 2021-12-01 13:20:32,637]\u001B[0m Trial 1 finished with value: 0.8458848061596351 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 200, 'learning_rate': 0.06788936356049051, 'l2_leaf_reg': 19.178848802339562, 'bootstrap_type': 'Bayesian', 'random_strength': 7, 'max_bin': 20, 'max_depth': 11, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 2, 'one_hot_max_size': 5, 'bagging_temperature': 8.653312871934858}. Best is trial 1 with value: 0.8458848061596351.\u001B[0m\n",
      "\u001B[32m[I 2021-12-01 13:20:38,908]\u001B[0m Trial 2 finished with value: 0.8678978879359097 and parameters: {'loss_function': 'Logloss', 'iterations': 200, 'learning_rate': 0.16197649270880024, 'l2_leaf_reg': 0.00032950837660805887, 'bootstrap_type': 'Bernoulli', 'random_strength': 1, 'max_bin': 20, 'max_depth': 2, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 3, 'one_hot_max_size': 12, 'subsample': 0.48281283489148485}. Best is trial 2 with value: 0.8678978879359097.\u001B[0m\n",
      "\u001B[32m[I 2021-12-01 13:20:54,608]\u001B[0m Trial 3 finished with value: 0.8704508964409955 and parameters: {'loss_function': 'Logloss', 'iterations': 500, 'learning_rate': 0.20789147289515256, 'l2_leaf_reg': 1.5182207444827891e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 1, 'max_bin': 6, 'max_depth': 2, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 7, 'one_hot_max_size': 12, 'bagging_temperature': 1.9649020176256882}. Best is trial 3 with value: 0.8704508964409955.\u001B[0m\n",
      "\u001B[32m[I 2021-12-01 13:29:13,528]\u001B[0m Trial 4 finished with value: 0.8172041082625957 and parameters: {'loss_function': 'Logloss', 'iterations': 1000, 'learning_rate': 0.040333992488965006, 'l2_leaf_reg': 2.3890937364388924, 'bootstrap_type': 'Bayesian', 'random_strength': 6, 'max_bin': 6, 'max_depth': 9, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 4, 'one_hot_max_size': 12, 'bagging_temperature': 3.979049987096378}. Best is trial 3 with value: 0.8704508964409955.\u001B[0m\n",
      "\u001B[32m[I 2021-12-01 13:31:35,341]\u001B[0m Trial 5 finished with value: 0.8523077596837422 and parameters: {'loss_function': 'Logloss', 'iterations': 200, 'learning_rate': 0.24236369833723986, 'l2_leaf_reg': 0.0011394128811327445, 'bootstrap_type': 'Bayesian', 'random_strength': 10, 'max_bin': 8, 'max_depth': 14, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 8, 'one_hot_max_size': 5, 'bagging_temperature': 7.161553698271376}. Best is trial 3 with value: 0.8704508964409955.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 6\n",
      "Best trial:\n",
      "  Value: 0.8704508964409955\n",
      "  Params: \n",
      "    loss_function: Logloss\n",
      "    iterations: 500\n",
      "    learning_rate: 0.20789147289515256\n",
      "    l2_leaf_reg: 1.5182207444827891e-05\n",
      "    bootstrap_type: Bayesian\n",
      "    random_strength: 1\n",
      "    max_bin: 6\n",
      "    max_depth: 2\n",
      "    grow_policy: Lossguide\n",
      "    min_data_in_leaf: 7\n",
      "    one_hot_max_size: 12\n",
      "    bagging_temperature: 1.9649020176256882\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a \"trial\" object and specify we want to MAXIMIZE the value being returned by the obj function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Running 100 trials, setting a timeout value of 15 minutes to prevent my computer from exploding.\n",
    "study.optimize(objective, n_trials=100, timeout=900)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "trial = study.best_trial\n",
    "\n",
    "# \"Prettify\" our trial results\n",
    "print(\"Best trial:\")\n",
    "\n",
    "# Print metric value achieved from best trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "# Print all parameters from best trial\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<catboost.core.CatBoostClassifier at 0x12708fd90>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model with the parameters from our best trial\n",
    "final_model = CatBoostClassifier(verbose=False, **trial.params)\n",
    "final_model.fit(X_train_os, y_train_os)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Results\n",
      "Accuracy: 0.89\n",
      "Precision: 0.47\n",
      "Recall: 0.83\n",
      "F1: 0.60\n",
      "ROC AUC: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Show custom metrics\n",
    "final_results = custom_metric(y_test, final_model.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "I chose to optimize on ROC score here, since I'm planning on using the predicted probabilities that a person has undiagnosed ADHD. We gained recall and AUC, however we lost more precision after tuning. We can see that the test set only missed **133 kids** that were diagnosed with ADHD.\n",
    "\n",
    "I would say this is the prefered scenario. If a child is identified as having undiagnosed ADHD, and they don't, then the worst that would happen is they'd see a doctor and have their worries put to rest. However, if the model says a child doesn't have ADHD, and later they **do** have ADHD, then that means more time, frustration, and confusion before they get diagnosed. Not even to mention the frustration of being told that you don't have ADHD, only to find out later that you do."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}