{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Catboost Tuning\n",
    "## Summary\n",
    "In this notebook I will primarily tune a Catboost model using [Optuna](https://optuna.readthedocs.io/en/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Data and Required Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from catboost import CatBoostClassifier, metrics, cv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, accuracy_score\n",
    "import optuna\n",
    "\n",
    "from functions import metrics as custom_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Training Data\n",
    "X_train = pd.read_csv('../Data/train/X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('../Data/train/y_train.csv', index_col=0)\n",
    "\n",
    "# Testing Data\n",
    "X_test = pd.read_csv('../Data/test/X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('../Data/test/y_test.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Currently the classes are labelled as \"1\" as dignosed ADHD and \"2\" as not diagnosed, but models seem to dislike this.\n",
    "testing = {2: 0, 1: 1}\n",
    "labels = y_train.replace(testing)\n",
    "test_labels = y_test.replace(testing)\n",
    "\n",
    "# This cell will be moved into the \"Data cleaning\" notebook in the future."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Initiate Over sampler\n",
    "ros = RandomOverSampler(random_state=15)\n",
    "\n",
    "# Applying ONLY to training set to prevent data leakage.\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizing with the Optuna\n",
    "For the hyperparameter tuning process, I'll be using the [Optuna](https://optuna.readthedocs.io/en/stable/index.html) library. I'll also be making use of Catboost's cross-validation in selecting the best model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Optuna requires us to define the \"objective function\". Which will be called upon each iteration during our \"trials\"\n",
    "def objective(trial):\n",
    "    # Dict of Parameters to check\n",
    "    param = {\n",
    "        # Metric used for model optimization\n",
    "        'loss_function':trial.suggest_categorical('loss_function', ['Logloss', 'CrossEntropy']),\n",
    "\n",
    "        # The maximum number of trees that can be built.\n",
    "        'iterations':trial.suggest_categorical('iterations', [100,200,300,500,1000]),\n",
    "\n",
    "        # learning rate for gradient descent calculations.\n",
    "        'learning_rate':trial.suggest_float(\"learning_rate\", 0.001, 0.3),\n",
    "\n",
    "        # Coefficient at the L2 regularization term of the cost function.\n",
    "        'l2_leaf_reg': trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 100),\n",
    "\n",
    "        # Affects the speed and regularization of tree\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "\n",
    "        # The amount of randomness to use for scoring splits.\n",
    "        'random_strength':trial.suggest_int(\"random_strength\", 1,10),\n",
    "\n",
    "        # The number of splits for numerical features.\n",
    "        'max_bin':trial.suggest_categorical('max_bin', [4,5,6,8,10,20,30]),\n",
    "\n",
    "        # Allowed depth of tree.\n",
    "        \"depth\": trial.suggest_int(\"max_depth\", 2,16),\n",
    "\n",
    "        # Defines how to perform greedy tree construction.\n",
    "        'grow_policy':trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
    "\n",
    "        # The minimum number of training samples in a leaf.\n",
    "        'min_data_in_leaf':trial.suggest_int(\"min_data_in_leaf\", 1,10),\n",
    "\n",
    "        # Only OHE encodes features if the number of unique values will be <= the parameter vale.\n",
    "        'one_hot_max_size':trial.suggest_categorical('one_hot_max_size', [5,10,12,100]),\n",
    "    }\n",
    "\n",
    "    # Certain parameters are \"subparameters\" and can only be set if their parent parameter has a certain value.\n",
    "\n",
    "    # Bootstrap types\n",
    "    if param['bootstrap_type'] == \"Bayesian\":\n",
    "\n",
    "        # Use Baysesian bootstrapping to assign random weights to objects.\n",
    "        param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "\n",
    "    elif param['bootstrap_type'] in ['Bernoulli', 'MVS']:\n",
    "        # Sample rate for bagging using Bernoulli/MVS type\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n",
    "\n",
    "    # Grow policy params\n",
    "    if param['grow_policy'] != 'SymmetricTree':\n",
    "\n",
    "        # The minimum number of training samples in a leaf.\n",
    "        param['min_data_in_leaf'] = trial.suggest_int('min_data_in_leaf', 1, 10)\n",
    "\n",
    "        if param['grow_policy'] == 'LossGuide':\n",
    "\n",
    "            # The maximum number of leafs in the tree.\n",
    "            param['max_leaves'] = trial.suggest_int('max_leaves', 16, 64)\n",
    "\n",
    "\n",
    "    # Creates the trial model with parameters specified above.\n",
    "    trial_model = CatBoostClassifier(**param)\n",
    "\n",
    "    # Fit the training model on training data\n",
    "    trial_model.fit(X_train_os,\n",
    "                    y_train_os,\n",
    "                    eval_set=[(X_test, test_labels)],\n",
    "                    verbose=0, # Stops Catboost from printing training results.\n",
    "                    early_stopping_rounds=10 # Specify rounds of no improvement needed before stopping\n",
    "                    )\n",
    "\n",
    "    # Create predictions for test set\n",
    "    preds = trial_model.predict(X_test)\n",
    "\n",
    "    # Calculate recall score\n",
    "    accuracy = accuracy_score(test_labels, preds)\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2021-11-30 12:18:38,420]\u001B[0m A new study created in memory with name: no-name-4a60b0f6-f9e0-40dc-812e-e809e240a07e\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:19:12,229]\u001B[0m Trial 0 finished with value: 0.9033308289506636 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 1000, 'learning_rate': 0.28358759379199844, 'l2_leaf_reg': 3.302782404194402e-07, 'bootstrap_type': 'MVS', 'random_strength': 6, 'max_bin': 5, 'max_depth': 16, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 7, 'one_hot_max_size': 5, 'subsample': 0.8109651242491138}. Best is trial 0 with value: 0.9033308289506636.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:19:38,485]\u001B[0m Trial 1 finished with value: 0.8787878787878788 and parameters: {'loss_function': 'Logloss', 'iterations': 100, 'learning_rate': 0.06729744504312246, 'l2_leaf_reg': 1.705294441579827e-07, 'bootstrap_type': 'MVS', 'random_strength': 8, 'max_bin': 20, 'max_depth': 11, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 5, 'one_hot_max_size': 5, 'subsample': 0.7783839289644973}. Best is trial 0 with value: 0.9033308289506636.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:19:51,750]\u001B[0m Trial 2 finished with value: 0.9034560480841473 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 100, 'learning_rate': 0.09104394518368454, 'l2_leaf_reg': 0.1200340364093349, 'bootstrap_type': 'Bernoulli', 'random_strength': 6, 'max_bin': 10, 'max_depth': 9, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 6, 'one_hot_max_size': 5, 'subsample': 0.8980746450350046}. Best is trial 2 with value: 0.9034560480841473.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:20:06,147]\u001B[0m Trial 3 finished with value: 0.9090909090909091 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 500, 'learning_rate': 0.16227261092031872, 'l2_leaf_reg': 2.352863583416497e-05, 'bootstrap_type': 'MVS', 'random_strength': 4, 'max_bin': 10, 'max_depth': 5, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 9, 'one_hot_max_size': 100, 'subsample': 0.39066323663257196}. Best is trial 3 with value: 0.9090909090909091.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:20:37,459]\u001B[0m Trial 4 finished with value: 0.8685199098422239 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 300, 'learning_rate': 0.048632156826959566, 'l2_leaf_reg': 16.424567171648317, 'bootstrap_type': 'Bayesian', 'random_strength': 2, 'max_bin': 5, 'max_depth': 4, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 10, 'one_hot_max_size': 5, 'bagging_temperature': 9.704093995369035}. Best is trial 3 with value: 0.9090909090909091.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:20:48,101]\u001B[0m Trial 5 finished with value: 0.8933132982719759 and parameters: {'loss_function': 'Logloss', 'iterations': 100, 'learning_rate': 0.2429247514636458, 'l2_leaf_reg': 0.0009180638230172947, 'bootstrap_type': 'Bernoulli', 'random_strength': 7, 'max_bin': 4, 'max_depth': 7, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 1, 'one_hot_max_size': 5, 'subsample': 0.1855082778681805}. Best is trial 3 with value: 0.9090909090909091.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:20:59,247]\u001B[0m Trial 6 finished with value: 0.8648885549711997 and parameters: {'loss_function': 'Logloss', 'iterations': 1000, 'learning_rate': 0.28910383133405454, 'l2_leaf_reg': 1.1272443905226674e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 4, 'max_bin': 10, 'max_depth': 10, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 7, 'one_hot_max_size': 5, 'bagging_temperature': 1.847350074934141}. Best is trial 3 with value: 0.9090909090909091.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:22:02,299]\u001B[0m Trial 7 finished with value: 0.9117205108940646 and parameters: {'loss_function': 'Logloss', 'iterations': 200, 'learning_rate': 0.25904642561640384, 'l2_leaf_reg': 0.015191844477911137, 'bootstrap_type': 'MVS', 'random_strength': 2, 'max_bin': 6, 'max_depth': 12, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 2, 'one_hot_max_size': 12, 'subsample': 0.8728188532897176}. Best is trial 7 with value: 0.9117205108940646.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:24:09,820]\u001B[0m Trial 8 finished with value: 0.9127222639619333 and parameters: {'loss_function': 'Logloss', 'iterations': 200, 'learning_rate': 0.26020701726678475, 'l2_leaf_reg': 0.0690809877792811, 'bootstrap_type': 'Bayesian', 'random_strength': 5, 'max_bin': 30, 'max_depth': 13, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 2, 'one_hot_max_size': 10, 'bagging_temperature': 3.3015176108026365}. Best is trial 8 with value: 0.9127222639619333.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:24:54,185]\u001B[0m Trial 9 finished with value: 0.8817931379914851 and parameters: {'loss_function': 'CrossEntropy', 'iterations': 300, 'learning_rate': 0.2799974800748025, 'l2_leaf_reg': 2.8060628760334467e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 9, 'max_bin': 10, 'max_depth': 13, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 1, 'one_hot_max_size': 5, 'bagging_temperature': 2.2879760139909724}. Best is trial 8 with value: 0.9127222639619333.\u001B[0m\n",
      "\u001B[32m[I 2021-11-30 12:35:17,712]\u001B[0m Trial 10 finished with value: 0.913473578762835 and parameters: {'loss_function': 'Logloss', 'iterations': 200, 'learning_rate': 0.19291655719205986, 'l2_leaf_reg': 95.18304430860879, 'bootstrap_type': 'Bayesian', 'random_strength': 10, 'max_bin': 30, 'max_depth': 16, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 3, 'one_hot_max_size': 10, 'bagging_temperature': 5.977019071447322}. Best is trial 10 with value: 0.913473578762835.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 11\n",
      "Best trial:\n",
      "  Value: 0.913473578762835\n",
      "  Params: \n",
      "    loss_function: Logloss\n",
      "    iterations: 200\n",
      "    learning_rate: 0.19291655719205986\n",
      "    l2_leaf_reg: 95.18304430860879\n",
      "    bootstrap_type: Bayesian\n",
      "    random_strength: 10\n",
      "    max_bin: 30\n",
      "    max_depth: 16\n",
      "    grow_policy: Depthwise\n",
      "    min_data_in_leaf: 3\n",
      "    one_hot_max_size: 10\n",
      "    bagging_temperature: 5.977019071447322\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a \"trial\" object and specify we want to MAXIMIZE the value being returned by the obj function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Running 100 trials, setting a timeout value of 15 minutes to prevent my computer from exploding.\n",
    "study.optimize(objective, n_trials=100, timeout=900)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "trial = study.best_trial\n",
    "\n",
    "# \"Prettify\" our trial results\n",
    "print(\"Best trial:\")\n",
    "\n",
    "# Print metric value achieved from best trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "# Print all parameters from best trial\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/y8/vq429chs6djb0hl0y0zp5fg00000gn/T/ipykernel_14085/3595234434.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Create a model with the parameters from our best trial\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mfinal_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCatBoostClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mfinal_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train_os\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train_os\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/catboost/core.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   4673\u001B[0m             \u001B[0mCatBoostClassifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_is_compatible_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'loss_function'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4674\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4675\u001B[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001B[0m\u001B[1;32m   4676\u001B[0m                   \u001B[0meval_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogging_level\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_description\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose_eval\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric_period\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4677\u001B[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/catboost/core.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   1995\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mlog_fixup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_cout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_cerr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1996\u001B[0m             \u001B[0mplot_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_get_train_dir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1997\u001B[0;31m             self._train(\n\u001B[0m\u001B[1;32m   1998\u001B[0m                 \u001B[0mtrain_pool\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1999\u001B[0m                 \u001B[0mtrain_params\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"eval_sets\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/Capstone/lib/python3.9/site-packages/catboost/core.py\u001B[0m in \u001B[0;36m_train\u001B[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001B[0m\n\u001B[1;32m   1426\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1427\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_clear_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1428\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_object\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_clear_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_object\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0minit_model\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1429\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_trained_model_attributes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1430\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m_catboost.pyx\u001B[0m in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_catboost.pyx\u001B[0m in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Create a model with the parameters from our best trial\n",
    "final_model = CatBoostClassifier(verbose=False, **trial.params)\n",
    "final_model.fit(X_train_os, y_train_os)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show custom metrics\n",
    "final_results = custom_metric(test_labels, final_model.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}